本章内容：
	K-均值聚类算法
	对聚类得到的簇进行后处理
	对二分K-均值聚类算法
	对地理位置进行聚类
聚类是一种无监督的学习，它将相似的对象归到同一个簇中，它有点像全自动分类。聚类方法几乎可以应用于所有对象，簇内的对象越相似，聚类的效果越好，之所以称之为K-均值是因为他们可以发现k个不同的簇，且每个簇中的中心采用簇中所含值得均值计算而成。

簇识别：
	簇识别给出聚类结果的含义，假定有一些数据，现在将相似数据归到一类，簇识别会告诉我们这些簇到底都是些什么。聚类与分类的最大不同在于，分类的目标事先已知，而聚类则不一样，因为其产生的结果与分类相同，而只是类别没有预先定义，聚类有时也被称为无监督分类。聚类分析图将相似对象归入同一簇，将不相似对象归到不同簇，相似这一概念取决于所选择的相似度计算方法。

10.1 K-均值聚类算法
	K-均值聚类
	优点：容易实现
	缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢
	适用数据类型：数值型数据
K-均值是发现给定数据集的k个簇的算法。簇个数k是用户给定的，每一个簇通过其质心，即簇中所有点的中心来描述
k-均值算法的工作流程是这样的，首先随机确定K个初始点作为质心，然后将数据集中的每个点分配到一个簇中，具体来将，为每个点找距离其最近的质心，并将其分配给该质心所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均值。

K-均值聚类的一般流程
（1）收集数据：使用任意方法
（2）准备数据：需要数值型数据来计算距离，也可以将标称型数据映射为二值型数据再用与距离计算。
（3）分析数据：使用任意方法
（4）训练算法：不适用于无监督学习，即无监督学习没有训练过程
（5）测试算法：应用聚类算法、观察结果。可以使用量化的误差指标如误差平方和来评价算法的结果。
（6）使用算法：可以用于所希望的任何应用，通常情况下，簇质心 可以代表整个簇的数据来做出决策

10.2使用后处理来提高聚类性能
如何知道k的选择是否正确，如何知道生成的簇比较好，在包含簇分配结果的矩阵中保存着每个点的误差，即该点到簇质心的距离平方值。K-均值算法收敛但聚类效果较差的原因是，K-均值算法收敛到了局部最小值，而非全局最小值

一种度量聚类效果的指标是SSE（误差平方和），SSE越小表示数据点越接近于他们的质心，聚类效果也越好，因为对误差取了平方，因此更加重视那些远离中心的点，一种肯定可以降低SSE值得方法是增加簇的个数，但这违背了聚类的目标。聚类的目标是保持簇数目不变的情况下提高簇的质量。

10.3二分K-均值算法
为克服K-均值算法收敛于局部最小值的问题，有人提出了另一个称为二分K-均值的算法。该算法首先将所有点作为一个簇，然后将簇一分为二，之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值，上述基于SSE的划分过程不断重复，直到得到用于指定的簇数目为止。

10.4示例：对地图上的点进行聚类
示例：对于地理数据应用二分K-均值算法
（1）收集数据：使用Yahoo!PlaceFinder API收集数据
（2）准备数据：只保留经纬度信息
（3）分析数据：使用Matplotlib来构建一个二维数据图，其中包括簇与地图
（4）训练算法：训练不适用无监督学习
（5）测试算法：使用10.4中的biKmeans()函数
（6）使用算法：最后的输出是包含簇及簇中心的地图

终止与获取yahoo数据



