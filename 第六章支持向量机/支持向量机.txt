本章内容：
	简单介绍支持向量机
	使用SMO进行优化
	利用核函数对数据进行空间转换
	将SVM和其他分类器进行对比
SVM有很多实现，但是本章只关注最流行的一种实现，即序列最小优化（SMO）算法，在此之后将介绍如何使用一种称为核函数的方式将SVM扩展到更多的数据集上，最后回顾第一张中手写识别的例子，并考察能否通过SVM来提高识别的效果。

6.1 基于最大间隔分割数据
	支持向量机
优点：泛化错误率低，计算开销不大，结果易解释
缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题
适用数据类型：数值型和标称型数据
	超平面：分类的决策边界，数据点离决策边界越远，那么最后的结果也就越可信
	间隔：这些点到分隔面的距离
	支持向量：就是离分隔超平面最近的那些点，接下来要试着最大化支持向量到分隔面的距离，需要找到此问题的优化求解方法。

6.2寻找最大间隔
	分隔超平面的形式可以写成W**T*X + b,要计算点A到分隔超平面的距离，就必须给出点到分隔面的法线或垂直线的长度，该值为|W**T*A+b|/||W||,这里的b类似于Logistic回归中的截距W0。

	由于所有数据都不那么"干净"，这时我们就可以通过引入所谓松弛变量，来允许有些数据点可以处于分割面的错误一侧

6.SVM应用的一般框架
	SVM的一般流程：
	（1）收集数据：可以使用任意方法
	（2）准备数据：需要数值型数据
	（3）分析数据：有助于可视化超平面
	（4）训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优
	（5）测试算法：十分简单的计算过程就可以实现
	（6）使用算法：几乎所有分类问题都可以使用SVM,值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码做一些修改。

在几百个点组成的小规模的数据集上，简化版的运行是没有什么问题的，但是在更大的数据集上的运行速度就会变慢。下面讨论PlattSMO,在这两个版本中，实现alpha的更改和代数运算的优化环节一模一样，在优化过程中，唯一的不同就是选择alpha的方式，完整的Platt SMO算法应用了一些能够提速的启发方法。

6.5在复杂数据上应用核函数：
	核函数将数据转换成易于分类器理解的形式。然后介绍一种称为径向基函数的最流行的核函数。
	SVM优化中有一个特别好的地方就是，所有的运算都可以写成内积（也称为点积），向量的内积指的是两个向量相乘，之后得到单个标量或者数值。我们可以把内积运算替换成核运算，而不必做简化处理。将内积替换成核函数的方式被称为核技巧
径向基函数是SVM中常用的一个核函数。径向基函数是一个采用向量作为自变量作为自变量的函数，能够基于向量距离运算输出一个标量，这个距离可以是<0,0>向量或者其他向量开始计算的距离。

6.6示例：手写识别问题回顾
	示例：给予SVM的数字识别
	（1）收集数据：提供的文本文件
	（2）准备数据：基于二值图像构造向量
	（3）分析数据：对于图像向量进行目测
	（4）训练算法：采用两种不同的核函数，并对径向量核函数采用不同的设置来运行SMO算法
	（5）测试算法：编写一个函数来测试不同的核函数并计算错误率
	（6）使用算法：一个图像识别的完整应用还需要一些图像处理的知识，这里并不打算深入介绍。




















