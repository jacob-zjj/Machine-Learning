{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词表到向量的转换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词表到向量的转换函数\n",
    "def loadDataSet():\n",
    "    postingList = [\n",
    "        ['my','dog','has','flea','problems','help','please'],\n",
    "        ['mybe','not','take','him','to','dog','park','stupid'],\n",
    "        ['my','dalmation','is','so','cute','I','love','him'],\n",
    "        ['stop','posting','stupid','worthless','garbage'],\n",
    "        ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "        ['qiut','buying','worthless','dog','food','stupid']\n",
    "    ]\n",
    "    classVec = [0,1,0,1,0,1]#1代表侮辱性文字，0代表正常言论\n",
    "    return postingList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vacabSet = set([])\n",
    "    for ducument in dataSet:\n",
    "        vacabSet = vacabSet | set(ducument)\n",
    "    return vacabSet\n",
    "\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    vocabList = list(vocabList)\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" %word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts,listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建非重复词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myVocabList = createVocabList(listOPosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setOfWords2Vec(myVocabList,listOPosts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练算法：从词向量计算概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯分类训练函数\n",
    "from numpy import *\n",
    "def trainNBO(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    # 初始化概率\n",
    "    p0Num = zeros(numWords)\n",
    "    p1Num = zeros(numWords)\n",
    "    p0Denom = 0.0\n",
    "    p1Denom = 0.0\n",
    "    # 向量相加\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num/p1Denom\n",
    "    p0Vect = p0Num/p0Denom\n",
    "    return p0Vect,p1Vect,pAbusive                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myVocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到信息矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V,p1V,pAV = trainNBO(trainMat,listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.125     ,\n",
       "       0.        , 0.04166667, 0.04166667, 0.        , 0.        ,\n",
       "       0.        , 0.08333333, 0.04166667, 0.04166667, 0.        ,\n",
       "       0.04166667, 0.        , 0.04166667, 0.04166667, 0.        ,\n",
       "       0.04166667, 0.        , 0.        , 0.04166667, 0.        ,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.        ,\n",
       "       0.04166667, 0.04166667])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05263158, 0.        , 0.        , 0.05263158, 0.05263158,\n",
       "       0.05263158, 0.05263158, 0.        , 0.        , 0.05263158,\n",
       "       0.05263158, 0.05263158, 0.10526316, 0.        , 0.05263158,\n",
       "       0.        , 0.05263158, 0.05263158, 0.        , 0.15789474,\n",
       "       0.        , 0.        , 0.        , 0.05263158, 0.10526316,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I',\n",
       " 'ate',\n",
       " 'buying',\n",
       " 'cute',\n",
       " 'dalmation',\n",
       " 'dog',\n",
       " 'flea',\n",
       " 'food',\n",
       " 'garbage',\n",
       " 'has',\n",
       " 'help',\n",
       " 'him',\n",
       " 'how',\n",
       " 'is',\n",
       " 'licks',\n",
       " 'love',\n",
       " 'mr',\n",
       " 'my',\n",
       " 'mybe',\n",
       " 'not',\n",
       " 'park',\n",
       " 'please',\n",
       " 'posting',\n",
       " 'problems',\n",
       " 'qiut',\n",
       " 'so',\n",
       " 'steak',\n",
       " 'stop',\n",
       " 'stupid',\n",
       " 'take',\n",
       " 'to',\n",
       " 'worthless'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯分类函数\n",
    "from math import log\n",
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    # 这里的相乘指的是两个向量相称的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘\n",
    "    # 然后再将第二个元素相乘，依次类推\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "from numpy import array\n",
    "def testingNB():\n",
    "    list0posts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(list0posts)\n",
    "    trainMat = []\n",
    "    for postinDoc in list0posts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb = trainNBO(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love','my','dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,\"classify as: \",classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "\n",
    "    testEntry = ['stupid','garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,\"classify as: \",classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classify as:  0\n",
      "['stupid', 'garbage'] classify as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regEx = re.compile(r'\\W*')\n",
    "# mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3.6.5fille\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "listOfTokens = regEx.split(mySent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon',\n",
       " '']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除空字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok for tok in listOfTokens if len(tok)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将字符串全部转换成小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in listOfTokens if len(tok)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集中一封完整的电子邮件的实际处理结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "emailText = open(r'F:\\Deeplearning\\机器学习实战10月25日\\MLAdata\\Ch04\\email\\ham\\6.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello,\\n\\nSince you are an owner of at least one Google Groups group that uses the customized welcome message, pages or files, we are writing to inform you that we will no longer be supporting these features starting February 2011. We made this decision so that we can focus on improving the core functionalities of Google Groups -- mailing lists and forum discussions.  Instead of these features, we encourage you to use products that are designed specifically for file storage and page creation, such as Google Docs and Google Sites.\\n\\nFor example, you can easily create your pages on Google Sites and share the site (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=174623) with the members of your group. You can also store your files on the site by attaching files to pages (http://www.google.com/support/sites/bin/answer.py?hl=en&answer=90563) on the site. If you抮e just looking for a place to upload your files so that your group members can download them, we suggest you try Google Docs. You can upload files (http://docs.google.com/support/bin/answer.py?hl=en&answer=50092) and share access with either a group (http://docs.google.com/support/bin/answer.py?hl=en&answer=66343) or an individual (http://docs.google.com/support/bin/answer.py?hl=en&answer=86152), assigning either edit or download only access to the files.\\n\\nyou have received this mandatory email service announcement to update you about important changes to Google Groups.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emailText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试算法：使用朴素贝叶斯进行交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  文件解析及完整的垃圾邮件测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据切分和转换\n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*',bigString)\n",
    "    return [tok.lower() for tok in  listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本检测\n",
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = []\n",
    "    fullText = []\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open(r'F:\\Deeplearning\\机器学习实战10月25日\\MLAdata\\Ch04\\email\\spam\\%d.txt' %i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open(r'F:\\Deeplearning\\机器学习实战10月25日\\MLAdata\\Ch04\\email\\ham\\%d.txt' %i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.append(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []\n",
    "    import random\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in  trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNBO(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:\n",
    "        wordVector = setOfWords2Vec(vocabList,docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print(\"the error rate is : \",float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is :  0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3.6.5fille\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following command must be run outside of the IPython shell:\n",
      "\n",
      "    $ pip install feedparser\n",
      "\n",
      "The Python package manager (pip) can only be used from outside of IPython.\n",
      "Please reissue the `pip` command in a separate terminal or command prompt.\n",
      "\n",
      "See the Python documentation for more information on how to install packages:\n",
      "\n",
      "    https://docs.python.org/3/installing/\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
