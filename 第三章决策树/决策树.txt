本章内容：
1、决策树建简介
2、在数据集中度量一致性
3、使用递归构造决策树
4、使用Matplotlib绘制树形图


决策树的构造：
优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据
缺点：可能会产生过度匹配问题
适用数据类型：数值型和标称型

决策树的一般流程：
（1）收集数据：可以使用任何方法
（2）准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化
（3）分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期
（4）训练算法：构造树的数据结构
（5）预测算法：使用经验树计算错误率
（6）使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义

标称型：一般在有限的数据中取，而且只存在‘是’和‘否’两种不同的结果（一般用于分类）
数值型：可以在无限的数据中取，而且数值比较具体化，例如4.02,6.23这种值（一般用于回归分析）


我们采用ID3算法划分数据，而不采用二分法划分数据http://en.wikipedia.org/wiki/ID3_algorithm

信息增益：
	划分数据集的大原则是：将无序的数据变得更加有序。
	在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择
	集合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父劳徳香农	
	熵定义为信息的期望值

	熵值越高，则混合的数据也越多，我们可以在数据集中添加更多的分类，观察是如何变化的度量信息增益

划分数据：
	我们将对每个特征划分数据集的结果计算一次信息熵，然后判断哪个特征数据集是最好的划分方式。

递归创建决策树：
	基本原理:得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据，因此我们可以采取递归的方式划分数据，调用函数

	递归结束的条件：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有的实例都具有相同的分类，则得到一个叶子节点或者终止块。任何到达叶子节点的数据必然属于叶子节点的分类

	由于特征数目并不是每次在每次划分数据时都减少，因此这些算法在实际使用时可能引起一定的问题，目前我们并不需要考虑这个问题，只需要在算法开始运行前计算列的数目，查看算法是否使用了所有属性即可。如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决定叶子节点的分类

测试算法：使用决策树执行分类
	依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子节点；最后将测试数据定义为叶子节点所属的类型。

使用算法：决策树的存储（多少数据模型进行）
	构造决策树是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒钟的时间，如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，则可以很快完成。因此，为了节省计算时间，最好能够在每次执行分类时调用已经构造好的决策树。为了解决这个问题，需要使用Python模块pickle序列化对象。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。任何对象都可以执行序列化操作。字典对象也不例外。

示例：使用决策树预测隐形眼镜类型
	(1)收集数据：提供的文本数据
	(2)准备数据：解析tab键分隔的数据行。
	(3)分析数据：快速检查数据，确保正确地解析数据内容，使用createPlot()函数绘制最终的树形图
	(4)训练算法：使用3.1节的createTree()函数
	(5)测试算法：编写测试函数验证决策树可以正确分类给定的数据实例
	(6)使用算法：存储树的数据结构，以便下次使用时无需重新构造树。
	然而创建的决策树非常好的匹配了实验数据，然后这些匹配选项可能太多了。我们将这种问题称之为过度匹配。为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子节点，如果叶子结点只能增加少许信息，则可以删除该节点并把它加入其它叶子节点中。第九章将会学习另一个决策构造算法CART，本章使用的算法称为ID3
	