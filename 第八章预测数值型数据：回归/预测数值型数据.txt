8.1用线性回归找到最佳拟合直线：
	线性回归：
	（1）优点：结果易于理解，计算上不复杂
	（2）缺点：对非线性的数据拟合不好
	（3）适用数据类型：数值型和标称型数据

回归的一般方法：
	（1）收集数据：采用任意方法收集数据
	（2）准备数据：回归需要数值型数据，标称型数据将被转成二值型数据。
	（3）分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比
	（4）训练数据：找到回归系数
	（5）测试算法：使用R(2)或者预测值和数据的拟合度，来分析模型的效果
	（6）使用算法：使用回归，可以在给定输入的的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续型数据而不仅仅是离散的类别标签


8.2局部加权线性回归
线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方误差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果，所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。
其中一个方法就是局部加权线性回归方法

8.3示例：预测鲍鱼的年龄
鲍鱼年龄可以从鲍鱼壳的层数推算得到
简单线性回归达到了与局部加权线性回归类似的效果，这点也表明，必须在未知数据上比较效果才能选取到最佳模型。

8.4 缩减系数来“理解”数据
	如果数据的特征比样本点还多应该怎么办，是否还可以使用线性回归和之前的方法来做预测，答案是否定的，如果特征比样本点还多，也就是说输入数据的矩阵X不是满秩矩阵，非满秩矩阵在求逆时会出现问题
为了解决这个问题，统计学家引入了岭回归，

岭回归：
	岭回归就是在矩阵中加入x从而使得矩阵非奇异，岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。 通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学上也叫做缩减。

岭回归中的岭是什么？
	岭回归使用了单位矩阵乘以常数λ，我们观察其中的单位矩阵I,可以看到值I贯穿整个对角线，其余元素全是0，形象地，在0构成的平面上有有一条1组成的岭，这就是回归中的“岭”的由来

还有一些其它的缩减方法，如lasso，LAR，PCA回归以及子集选择等，与岭回归一样，这些方法不仅可以提高预测精确率，而且可以解释回归系数。

8.4.2lasso回归方法
	前向逐步回归：前向逐步回归算法可以得到与lasso差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有的权重都设为1，然后每一步所做的决策是对某个权重增加或减少误差。一开始，所有的权重都设为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值。

	前向逐步回归：
	数据标准化，使其分布满足0均值和单位方差
	在每轮迭代过程中：
		设置当前最小误差lowestError为正无穷
		对每个特征：
		       增大或缩小：
			改变一个系数得到一个新的W
			计算新W下的误差
			如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W将W设置为新的Wbest
		       将W设置为新的Wbest
	
	逐步线性回归算法的实际好处并不在于能够绘制一个很好的图，主要的有点在于它可以帮助人们理解现有的模型并作出改进。当构建了一个模型后，可以运行改算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集，最后，如果用于测试，该算法用每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择使误差最小的模型。

8.5权衡于偏差
	示例：用回归法预测乐高套装的价格
	（1）收集数据：用Google Shopping的API收集数据
	（2）准备数据：从返回的JSON数据中抽取价格
	（3）分析数据：可视化并观察数据
	（4）训练算法：构建不同的模型，采用逐步线性回归和直接的线性回归模型
	（5）测试算法：使用交叉验证来测试不同的模型，分析哪个小鬼最好
	（6）使用算法：这次练习目标就是为了生成数据模型


