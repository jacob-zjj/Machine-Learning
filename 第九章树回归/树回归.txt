CART算法
回归于模型树
树剪枝算法
Python中GUI的使用

当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法显得太难了。而且实际生活中很多问题都是非线性的，不可能使用全局全局线性模型来拟合任何数据。一种可行的方法是将数据集切分成很多分易建模的数据，然后利用线性回归技术来建模，数结构和回归法就相当有用。

本章首先介绍一个新的叫做CART(分类回归树)的树构建算法。该算法既可以用于分类还可以用于回归，因此非常值得学习。然后利用Python来构建并显示CART树，代码会保持足够的灵活性以便能用于多个问题当中。接着，利用CART算法构建回归树并介绍其中的树剪枝技术。之后引入了一个更高级的模型树算法，与回归树的做法(在每个节点上使用各自的均值做预测)不同，改算法需要在每个叶节点上都构建一个线性模型。在这些树的构建算法中有一些需要调整的参数，所以还会介绍如何使用Python中的Tkinter模型建立图形交互界面。最后，在该界面的辅助下分析参数对回归效果的影响。

9.1 复杂数据的局部性建模
决策树不断将数据切分成小数据集，直到所有目标变量完全相同，或者数据不能再切分为止。决策树是一种贪心算法，它在给定时间内做出最佳选择，但并不关心能否达到全局最优。
	优点：可以对复杂和非线性的数据建模。
	缺点：结果不易理解。
	适用数据类型：数值型和标称型数据。
第三章选择的是ID3构建算法，ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征值所有可能取值来切分，也就是说，如果一个特征有4种取值，那么数据将被切成4份，一旦按某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分过于迅速。另外一种方法是二元切分法，即每次把数据集切成两份，如果数据的某特征值等于切分要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先把连续型特征换成离散型，才能在ID3算法中使用，但这种转换过程会破坏连续型变量的内在性质，而使用二元切分法则易于对树构建过程进行调整以处理连续特征。具体的处理方式：如果特征值大于给定值就走左子树，否则就右子树。另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树一般是离线完成，时间并非需要重点关注的因素。
	树回归的一般方法：
	（1）收集数据：采用任意方法收集数据。
	（2）准备数据：需要数值型的数据，标称型数据应该映射成二值型数据。
	（3）分析数据：绘出数据的二维可视化显示结果，以字典方式生成树
	（4）训练算法：大部分时间都花费在叶节点数模型的构建上
	（5）测试算法：使用测试数据上的R^2值来分析模型的效果。
	（6）使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情

9.2 连续和离散型特征的树的构建
在树的构建中，需要解决多种类型的数据的存储问题，这里使用一部字典来存储树的数据结构，该字典将包含以下4个元素。
	待切分的特征。
	待切分的特征值。
	右子树。当不在需要切分的时候，也可以是单个值
	左子树。与右子树类似

9.3将CART算法用于回归
	为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。第三章使用树进行分类，会在给定节点时计算数据的混乱度，那么如何计算连续型数值的混乱度？事实上，在数据集上计算混乱度是非常简单的，首先计算所有数据的均值，然后计算每条数据的值到均值的差值，为了对正负差值同等看待，一般使用绝对值或平方值，来代替上述差值，上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一不同就是，方差是平方误差的均值（均方差），而这里需要的是平方误差的总值（总方差）。总方差可以通过均方差乘以数据集中样本点的个数来得到。

9.3.1构建树

9.4树剪枝
一棵树如果节点过多，表明该模型可能对数据进行了"过拟合"。那么，如何判断是否发生了过拟合？前面章节中使用了测试集上某种交叉技术来发现过拟合，决策树亦是如此。通过降低决策树的复杂度来避免过拟合的过程称为剪枝，其实本章前面已经进行了剪枝操作。另一种形式的剪枝需要使用测试集和训练集，称作后剪枝，本节将分析后剪枝的有效性，但首先看下预剪枝的不足之处。

9.4.1预剪枝
事实上我们常常甚至不确定到底需要寻找什么样的结果，这正是机器学习的结果，计算机应该可以给出总体的概貌。由于不需要用于指定参数，后剪枝是一个更理想化的剪枝方法。

9.4.2后剪枝
使用后剪枝方法需要将数据集分成测试集和训练集。首先指定参数，使得构建出的树足够大，足够复杂，便于剪枝。接下来从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低测试误差。如果是的话就合并

9.5模型树
用树来对数据建模，除了把也节点简单设定为长数值之外，还有一种方法是吧叶节点设定为分段线性函数，这里所谓的分段线性是指模型由多个线性片段组成。

决策树相比于其他机器学习算法的优势在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易理解。模型树的可解释性是它由于回归树的特点之一。另外，模型树也具有更高的预测准确度
前一章使用了标准的线性回归法，本章则使用了树回归法，下面将通过实例对二者进行比较，最后用函数corrcoef()来分析哪个模型是最优的

9.6示例：树回归与标准回归的比较

9.7使用Python的Tkinter库创建GUI
支持数据呈现和用户交互的方式就是构建一个图形用户界面（GUI）
	示例：利用GUI对回归树调优
	（1）收集数据：所提供的文本文件
	（2）准备数据：用Python解析上述文件，得到数值型数据。
	（3）分析数据：用Tkinter构建一个GUI来展示模型和数据。
	（4）训练算法：训练一棵回归树和一棵模型树，并与数据集一起展示出来
	（5）测试算法：这里不需要测试过程
	（6）使用算法：GUI使得人们可以在预剪枝时测试不同参数的影响，还可以帮助我们选择模型的类型。

9.7.1用Tkinter创建GUI












