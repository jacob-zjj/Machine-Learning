Logistic回归的一般模型：
	（1）收集数据：采用任意方法收集数据。
	（2）准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳
	（3）分析数据：采用任意方法对数据进行分析
	（4）训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数
	（5）测试算法：一旦训练步骤完成，分类将会很快
	（6）使用算法：首先我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定他们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。

基于Logistic回归和Sigmoid函数的分类：
	Logistic回归：
	优点：计算代价不高，易于理解和实现。
	缺点：容易欠拟合，分类精度可能不高。
	使用数据类型：数值型和标称型数据。
因此，为了实现Logistic回归分类器，我们可以在每个特征值上都乘以一个回归系数，然后把所有的结果值都相加，把这个总和代入Sigmoid函数中，进而得到一个范围在0-1之间的数值。任何大于0.5的被分入1类，小于0.5的被归入0类。所以，Logistic回归可以被看做是一种效率估计。
确定了分类器的函数形式后，现在的问题变成了：最佳回归系数是多少？如何确定它的大小？

梯度上升方法：
	我们介绍的第一个最优算法叫做梯度上升法。梯度上升法基于的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。

	梯度上升算法到达每个点都会重新估计移动的方向，从P0开始，计算完该点的梯度，函数就根据梯度移动到下一点P1。在P1点，梯度再次被重新计算，并沿着新的梯度方向移动到P2。如此循环迭代，知道满足停止条件，迭代的过程中梯度算子总是保证我们能选取到最佳的移动方向。
梯度下降方法：
	它与梯度上升算法是一样的，只是公式中加法需要变成减法。梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。


训练算法：使用梯度上升找到最佳参数
	该方法每次在更新回归系数时是对整个数据集进行遍历

训练算法：随机梯度上升
	梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理100个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进的方法就是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。

	可以看出随机梯度上升与梯度上升算法在代码上很相似，但也有一些区别：第一，后者的变量h和误差error都是向量，而前者则全是数值；第二，前者没有矩阵的转换过程，所有变量的数据类型都是Numpy数组。

随机梯度上升算法存在一定的问题：我们期望算法能避免来回波动，从而收敛到某个值。另外收敛速度也需要加快。因此需要改进随机梯度上升算法

示例：从疝气病症预测病马的死亡率

使用Logistic回归估计马疝病的死亡率：
	（1）收集数据：给定数据文件。
	（2）准备数据：用Python解析文本文件并填充缺失值。
	（3）分析数据：可视化并观察数据
	（4）训练算法：使用优化算法，找到最佳的系数
	（5）测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数
	（6）使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以做为留给读者一道习题

处理数据中的缺省值：
	1、使用可用特征的均值来填补缺省值
	2、使用特殊值来填补缺省值，如-1
	3、忽略有缺省值的样本
	4、使用相似样本的均值添补缺省值
	5、使用另外的机器学习算法预测缺省值

现在我们对要用的数据集进行预处理，使其可以顺利地进行分类算法。在预处理阶段需要做两件事，
	第一，所有缺失值必须用一个实数值来替换，因为我们使用的Numpy数据数据类型不允许包含缺省值。这里选择实数0来替换所有缺省值，恰好能适用于Logistic回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。回归系数的更新公式如下：
		weights = weights + alpha * error * range(dataMatrix[randIndex])	
	如果dataMaterix的某特征值对应值为0，那么该特征的系数将不做更新，即：
		weights = weights
		

	第二，如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们简单的做法是将该条数据丢弃，这是因为类别标签与特征标签不同，很难确定采用某个合适的值来替换。采用Logistic回归进行分类时是合理的，而如果采用KNN的时候则不行

测试算法：用Logistic回归进行分类
	