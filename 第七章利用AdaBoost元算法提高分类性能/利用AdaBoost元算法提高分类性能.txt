元运算是对其他算法进行组合的一种方式。

本章首先讨论不同分类器的集成方法，然后主要关注AdaBoost的最流行的元算法。再接下来我们就会建立一个单层决策树分类器，实际上它是一个单节点决策树。

基于数据集多重抽样的分类器：
	前面已经介绍了五种不同的的分类算法，他们各有缺点，我们自然可以将不同的分类器组合起来，而这种组合结果则被称为集成方法或者元算法。使用集成方法时会有多种形式：可以是不同算法的集成，也可以是不同设置下的集成，也可以是同一算法在不同设置下的集成，还可以是数据集不同部分分配给不同分类器之后的集成。接下来，我们将介绍基于同一种分类器多个不同实例的两种计算方法。

AdaBoost：
	优点：泛化错误率低，易于编码，可以应用在大部分分类器上，无参数调整。
	缺点：对高群点敏感
	适用数据类型：数值型和标称型数据

bagging：基于数据随机重抽样的分类器构建方法
	自举汇聚法，是从原始数据集选择S次后得到S个新数据集的一种技术。新数据集和原始数据集的大小相等，每个数据集都是通过在原始数据集中随机选择一个样本来进行替换而得到的。这里的替换就意味着可以多次选择同一样本，这一性质就允许新数据集中可以有重复的值，而原始数据集的某些值在新集合中则不再出现。
	在S个数据集建好之后，将某个学习算法分别作用于每个数据集就得到了S个分类器。当我们要对新数据进行分类时，就可以用这个S个分类器进行分类。与此同时，选择分类器投票结果中最多的类别作为最后的分类结果。

boosting:
	boosting是一种与bagging很类似的技术。不论是boosting还是bagging当中，所使用的多个分类器的类型都是一致的，但是在前者当中，不同的分类器是通过串行训练而获得的。

AdaBoost的一般流程：
	（1）收集数据：可以使用任意方法
	（2）准备数据：依赖所使用的弱分类器类型，本章使用的是单层次决策树，这种分类器可以处理任何数据类型，当然也可以使用任意分类器作为弱分类器，第二章到第六章中任一分类器都可以充当弱分类器，作为分类器，简单分类器的效果更好
	（3）分析数据：可以使用任意方法
	（4）训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器
	（5）测试算法：计算分类的错误率
	（6）使用算法：同SVM一样，AdaBoost预测两个类别中的一个。如果想把它应用到多个类别的场合，那么就要像多类SVM中的做法一样对AdaBoost进行修改

示例：在一个难数据集上应用AdaBoost
	（1）收集数据：提供的文本文件
	（2）准备数据：确保类别标签是+1和-1而非1和0
	（3）分析数据：手工检查数据
	（4）训练数据：在数据上，利用adaBoostTrainDs()函数训练出一系列的分类器
	（5）测试算法：我们拥有两个数据集，在不采用随机抽样的方法下，我们就会对AdaBoost和Logistic回归的结果进行完全对等的比较
	（6）使用算法：观察该测试例子上的错误率。不过，也可以构建一个Web网站，让驯马师输入马的症状然后预测马是否会死去
很多人认为，AdaBoost和SVM是监督机器学习中最强大的两种方式，实际上，这两者之间拥有不少相似之处，我们可以把弱分类器想象成SVM中的一个核函数，也可以按照最大化某个最小间隔的方式重写AdaBoost算法，而他们的不同就在于其所定义的间隔计算方式有所不同，因此导致的结果也不同，特别是在高维空间下，这两者之间的差异就会更加明显。

非均衡分类问题：
其它分类性能度量指标：正确率，召回率及ROC曲线：
	在二分类问题中，如果将一个正例判为正例，那么可以认为产生了一个真正例，如果对一个反例正确地判为反例，则认为产生了一个真反例

另一个用于度量分类中的非均衡性的工具是ROC曲线(ROC curve),ROC代表接收者操作特征。

基于代价函数的分类决策控制：
	除了调节分类器的阈值之外，我们还有一些其他可以用于处理非均衡分类代价的方法，其中的一种称为代价敏感的学习；提前知道分类器的代价值，知道了代价值，那么就可以选择付出最小代价的分类器

处理分均衡问题的数据抽样方法：
	另外一种针对分均衡问题调节分类器的方法，就是对分类器的训练数据进行改造。这可以通过欠抽样或者过抽样来实现。过抽样意味着复制样例，而欠抽样意味着删除样例。不管采用哪种方式，数据都会从原始形式改造为新形式，抽样过程则可以通过随机方式或者某个预定方式来实现

