KNN
简单的说，k-近邻算法采用测量不同特征值之间的距离方法进行分类

优点：精度高、对异常值不敏感、无数据输入假定
缺点：计算复杂度高、空间复杂度高。
适用数据范围：数值型和标称型

工作原理：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（近邻数据）的分类标签。一般来说我们只选择样本数据集中前K个最相似数据中出现次数最多的分类，作为新数据的分类。 

K-近邻算法的一般流程：
	1）收集数据：可以使用任何方法
	2）准备数据：距离计算所需要的数值，最好是结构化的数据格式
	3）分析数据：可以使用任何方法
	4）训练算法：此步骤不适用于k-近邻算法
	5）测试算法：计算错误率
	6）使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理

KNN算法，对未知类别属性的数据集中的每个点依次执行以下操作：
（1）计算已知类别数据集中的点与当前点之间的距离；
（2）按照距离递增次序排序
（3）选取与当前点距离最小的k个点
（4）确定前k个点所在类的出现频率
（5）返回当前k个点出现频率最高的类别作为当前点的预测分类

示例：在约会网站上使用K-近邻算法
（1）收集数据：提供文本文件
（2）准备数据：使用Python解析文本文件
（3）分析数据：使用Matplotlib画二维扩散图
（4）训练算法：此步骤不适用于k-近邻算法
（5）测试算法：使用海伦提供的部分数据作为测试数据样本
	 测试样本和非测试样本的区别在:测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误
（6）使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型

海伦收集约会数据已经有一段时间了，她把这些数据存放在文本文件datingTestSet.txt中，每个样本数据占据一行，总共有1000行。海伦的样本主要包含以下三种特征：
1、每年获得的飞行常客里程数
2、玩视频游戏所耗时间百分比
3、每周消费的冰淇淋公升数
通过实验得到了如果将第1种和第2中做为分类条件将会把数据分得更加明显

实验中我们发现，如果在求距离最小值时，采用平方和根求最小值时，每年获取的飞行常客里程数对于计算结果的影响将远远大于其他两列特征，但是其实这三列对于判断结果都很重要
	因此在处理这种不同取值范围的特征值是，我们通常采用的方法是将数值归一化，如将取值范围处理为0到1或-1到1之间。
	newvalue = (oldValue - min)/(max - min)
	虽然改变数值取值范围增加了分类器的复杂度，但为了得到准确结果，我们必须这样做。因此我们需要在KNN.py文件中增加一个函数autoNorm(),该函数可以自动将特征值转化为0到1的区间


测试算法：作为完整程序验证分类器
	通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确性（完美的分类器的错误率为0，而错误率为1.0的分类器不会给出任何正确的分类结果）


示列：手写识别系统
	（1）收集数据：提供文本文件
	（2）准备数据：编写函数classify0(),将图像格式转换为分类器使用的list格式
	（3）分析数据：在Python命令提示符中检查数据，确保它符合要求
	（4）训练算法：此步骤不适用于K-近邻算法
	（5）测试算法：编写函数使用提供的部分数据操作作为测试样本，测试样本与非测试样本的区别在于测试样本是已经完成分类的数据，如果测试数分类与实际分类不同，则标记为一个错误。
	（6）使用算法：本例没有完成此步骤
为了使用前面两个例子的分类器，我们必须将图像格式化处理为一个向量。我们将把一个32x32的二进制图像矩阵转换为1x1024的向量，这样前两节使用的分类器就可以处理数字图像信息了。


